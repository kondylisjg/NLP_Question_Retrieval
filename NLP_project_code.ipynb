{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CcI7p3rxKYg"
   },
   "source": [
    "# Data Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z7CoDpUjukAI"
   },
   "outputs": [],
   "source": [
    "#RUN ME!!!\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "balanced_file_path = '/content/drive/My Drive/NLP Final Project/balanced_df.csv'\n",
    "tech_file_path = '/content/drive/My Drive/NLP Final Project/tech_data.csv'\n",
    "\n",
    "\n",
    "try:\n",
    "    balanced_df = pd.read_csv(balanced_file_path)\n",
    "    tech_data = pd.read_csv(tech_file_path)\n",
    "    print(\"DataFrame loaded successfully from Google Drive.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Csv not found in Google Drive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6IFyidfzDqP1"
   },
   "source": [
    "## BoW with TF-IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBruucIFDv4T"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ef2edIvkFc4u"
   },
   "outputs": [],
   "source": [
    "def get_tfidf_embeddings(train_data, test_data):\n",
    "  # Create BOW\n",
    "  vectorizer = CountVectorizer()\n",
    "  bow = vectorizer.fit_transform(train_data)\n",
    "\n",
    "  # TF-IDF weighting\n",
    "  tfidf_transformer = TfidfTransformer()\n",
    "  tfidf = tfidf_transformer.fit_transform(bow)\n",
    "\n",
    "  # Get TF-IDF embeddings for test data\n",
    "  test_bow = vectorizer.transform(test_data)\n",
    "  test_tfidf = tfidf_transformer.transform(test_bow)\n",
    "  return test_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mnrhsBVNGA3o"
   },
   "outputs": [],
   "source": [
    "# Getting the P@1 accuracy\n",
    "def p_at_1_accuracy(cosine_distances_matrix, keyword_rows):\n",
    "  best_matches = np.argmin(cosine_distances_matrix, axis=1)\n",
    "  correct_indices = np.arange(len(keyword_rows))\n",
    "  correct_matches = (best_matches == correct_indices)\n",
    "  accuracy = np.mean(correct_matches)\n",
    "  print(f\"Top-1 Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-T4zCNY4TQs"
   },
   "outputs": [],
   "source": [
    "def p_at_1_accuracy_bow(cosine_distances_matrix, duplicates_df):\n",
    "  best_matches = np.argmin(cosine_distances_matrix, axis=1)\n",
    "  correct_matches = 0\n",
    "  for i, pred_idx in enumerate(best_matches):\n",
    "    if duplicates_df.index[i] == pred_idx:\n",
    "      correct_matches += 1\n",
    "  accuracy = correct_matches / len(duplicates_df)\n",
    "  print(f\"Top-1 Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eNSTOkJVI9Df"
   },
   "outputs": [],
   "source": [
    "# Getting the P@3 accuracy\n",
    "def p_at_3_accuracy(cosine_distances_matrix, keyword_rows):\n",
    "  top_3_matches = np.argsort(cosine_distances_matrix, axis=1)[:,:3]\n",
    "  correct_matches = np.array([i in top_3_matches[i] for i in range(len(keyword_rows))])\n",
    "  top_3_accuracy = np.mean(correct_matches)\n",
    "  print(f\"Top-3 Accuracy: {top_3_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "trglnvEI5i8C"
   },
   "outputs": [],
   "source": [
    "def p_at_3_accuracy_bow(cosine_distances_matrix, duplicates_df):\n",
    "  correct_matches = 0\n",
    "  for i in range(len(duplicates_df)):\n",
    "    top_3_matches = np.argsort(cosine_distances_matrix[i])[:3]\n",
    "    if duplicates_df.index[i] in top_3_matches:\n",
    "      correct_matches += 1\n",
    "  top_3_accuracy = correct_matches / len(duplicates_df)\n",
    "  print(f\"Top-3 Accuracy: {top_3_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB213uFrO_kh"
   },
   "outputs": [],
   "source": [
    "# Getting the MRR\n",
    "def mrr(cosine_distances_matrix, keyword_rows):\n",
    "  correct_ranks = []\n",
    "  for i in range(len(keyword_rows)):\n",
    "    sorted_indices = np.argsort(cosine_distances_matrix[i])\n",
    "    rank = np.where(sorted_indices == i)\n",
    "    correct_ranks.append(rank[0][0] + 1)\n",
    "  mrr = np.mean([1 / rank for rank in correct_ranks])\n",
    "  print(f\"MRR: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dbxBSOjS6TET"
   },
   "outputs": [],
   "source": [
    "def mrr_bow(cosine_distances_matrix, duplicates_df):\n",
    "  correct_ranks = []\n",
    "  for i in range(len(duplicates_df)):\n",
    "    sorted_indices = np.argsort(cosine_distances_matrix[i])\n",
    "    if duplicates_df.index[i] in sorted_indices:\n",
    "      rank = np.where(sorted_indices == duplicates_df.index[i])\n",
    "      correct_ranks.append(1 / (rank[0][0] + 1))\n",
    "    else:\n",
    "      correct_ranks.append(0)\n",
    "  mrr = np.mean(correct_ranks)\n",
    "  print(f\"MRR: {mrr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCwQm37u11om"
   },
   "outputs": [],
   "source": [
    "# Getting accuracy with tau threshold\n",
    "def tau_accuracy(cosine_distances_matrix, balanced_df):\n",
    "  threshold = 0.8\n",
    "  cosine_similarity_matrix = 1 - cosine_distances_matrix\n",
    "\n",
    "  row_accuracies = []\n",
    "  for idx, row in balanced_df.iterrows():\n",
    "    is_duplicate = row[\"is_duplicate\"]\n",
    "    similarity = cosine_similarity_matrix[idx, idx]\n",
    "\n",
    "    if is_duplicate == 1:\n",
    "      row_accuracies.append(similarity >= threshold)\n",
    "    else:\n",
    "      row_accuracies.append(similarity < threshold)\n",
    "\n",
    "  row_wise_accuracy = np.mean(row_accuracies)\n",
    "  print(f\"Overall Accuracy with threshold 0.8: {row_wise_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwNVguEYnFNG"
   },
   "source": [
    "For the tech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Z_RE-8M2Ozf"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(tech_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LC1jy5baFu-i"
   },
   "outputs": [],
   "source": [
    "train_questions = train_df[\"question1\"].tolist() + train_df[\"question2\"].tolist()\n",
    "test_questions = test_df[\"question1\"].tolist() + test_df[\"question2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWgnLtX3FwaP"
   },
   "outputs": [],
   "source": [
    "test_embeddings = get_tfidf_embeddings(train_questions, test_questions)\n",
    "\n",
    "question1_embeddings = test_embeddings[:len(test_df)]\n",
    "question2_embeddings = test_embeddings[len(test_df):]\n",
    "\n",
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RSeGRzVR33a7"
   },
   "outputs": [],
   "source": [
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmHhJXDcxcLT"
   },
   "outputs": [],
   "source": [
    "duplicates = test_df[test_df[\"is_duplicate\"] == 1]\n",
    "duplicate_indices = duplicates.index.tolist()\n",
    "cosine_distances_duplicates = cosine_distances_matrix[duplicate_indices, :]\n",
    "\n",
    "p_at_1_accuracy_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcTLGy_MeJBk"
   },
   "outputs": [],
   "source": [
    "p_at_3_accuracy_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kVyIQWWceRhl"
   },
   "outputs": [],
   "source": [
    "mrr_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7KB-_wNmlCN1"
   },
   "outputs": [],
   "source": [
    "tau_accuracy(cosine_distances_matrix, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gzz_s_cShgN8"
   },
   "source": [
    "For the general dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dOYkYJl56uTN"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(balanced_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IE44jBPg6wBo"
   },
   "outputs": [],
   "source": [
    "train_questions = train_df[\"question1\"].tolist() + train_df[\"question2\"].tolist()\n",
    "test_questions = test_df[\"question1\"].tolist() + test_df[\"question2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXga9vkS6082"
   },
   "outputs": [],
   "source": [
    "test_embeddings = get_tfidf_embeddings(train_questions, test_questions)\n",
    "\n",
    "question1_embeddings = test_embeddings[:len(test_df)]\n",
    "question2_embeddings = test_embeddings[len(test_df):]\n",
    "\n",
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orYqRKZ0pBwx"
   },
   "outputs": [],
   "source": [
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uh5bKVngea9u"
   },
   "outputs": [],
   "source": [
    "duplicates = test_df[test_df[\"is_duplicate\"] == 1]\n",
    "duplicate_indices = duplicates.index.tolist()\n",
    "cosine_distances_duplicates = cosine_distances_matrix[duplicate_indices, :]\n",
    "\n",
    "p_at_1_accuracy_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYp8TqKxpwfG"
   },
   "outputs": [],
   "source": [
    "p_at_3_accuracy_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgJcAZyoqRG6"
   },
   "outputs": [],
   "source": [
    "mrr_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EkXZ4WNkuToD"
   },
   "outputs": [],
   "source": [
    "tau_accuracy(cosine_distances_matrix, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI-FyAZMKbJ3"
   },
   "source": [
    "## Pre-Trained Word2Vec ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uN0b55gFL0pe"
   },
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(\"/content/drive/My Drive/NLP Final Project/GoogleNews-vectors-negative300.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CxvhxRZEBbK7"
   },
   "source": [
    "For the tech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fxyqh8TSMTFu"
   },
   "outputs": [],
   "source": [
    "all_questions = tech_data[\"question1\"].tolist() + tech_data[\"question2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucoG53o2ht4u"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ydFo4KHqUBRs"
   },
   "outputs": [],
   "source": [
    "def get_weighted_question_embedding(question, tfidf_vectorizer, tfidf, i, word2vec):\n",
    "  vocab_questions = tfidf_vectorizer.vocabulary_\n",
    "  question_words = question.split()\n",
    "  word_embeddings = []\n",
    "  weights = []\n",
    "\n",
    "  for word in question_words:\n",
    "    if word in vocab_questions and word in word2vec:\n",
    "      word_index = vocab_questions[word]\n",
    "      word_weight = tfidf[i, word_index]\n",
    "      word_embedding = word2vec[word]\n",
    "      word_embeddings.append(word_embedding * word_weight)\n",
    "      weights.append(word_weight)\n",
    "\n",
    "  if word_embeddings:\n",
    "    question_embedding = np.sum(word_embeddings, axis=0) / np.sum(weights)\n",
    "\n",
    "  else:\n",
    "    print(\"No words found\")\n",
    "    question_embedding = np.zeros(word2vec.vector_size)\n",
    "\n",
    "  return question_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ItCQFEtUSy0"
   },
   "outputs": [],
   "source": [
    "question1_embeddings = np.array([get_weighted_question_embedding(q, tfidf_vectorizer, tfidf, i, model)\n",
    "                                for i, q in enumerate(tech_data[\"question1\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvwXz8ESUVEh"
   },
   "outputs": [],
   "source": [
    "question2_embeddings = np.array([get_weighted_question_embedding(q, tfidf_vectorizer, tfidf, i + len(tech_data), model)\n",
    "                                for i, q in enumerate(tech_data[\"question2\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLEQrFZfUdbQ"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_5S7yTf8KIe"
   },
   "outputs": [],
   "source": [
    "duplicates = tech_data[tech_data[\"is_duplicate\"] == 1]\n",
    "duplicate_indices = duplicates.index.tolist()\n",
    "cosine_distances_duplicates = cosine_distances_matrix[duplicate_indices, :]\n",
    "\n",
    "p_at_1_accuracy_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SwaEOv-QUfLn"
   },
   "outputs": [],
   "source": [
    "p_at_3_accuracy_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YHtX8DMdHEr"
   },
   "outputs": [],
   "source": [
    "mrr_bow(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GOpIvUldOsC"
   },
   "outputs": [],
   "source": [
    "tau_accuracy(cosine_distances_matrix, tech_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cYiMi9DwXt4"
   },
   "source": [
    "For the general dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KOt8UH9wYyX"
   },
   "outputs": [],
   "source": [
    "duplicates = balanced_df[balanced_df[\"is_duplicate\"] == 1]\n",
    "all_questions = balanced_df[\"question1\"].tolist() + balanced_df[\"question2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ebvr20NbxRt-"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17JrV1q6j2SQ"
   },
   "outputs": [],
   "source": [
    "question1_embeddings = np.array([get_weighted_question_embedding(q, tfidf_vectorizer, tfidf, i, model)\n",
    "                                for i, q in enumerate(balanced_df[\"question1\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCG-TTCZkHYA"
   },
   "outputs": [],
   "source": [
    "question2_embeddings = np.array([get_weighted_question_embedding(q, tfidf_vectorizer, tfidf, i + len(balanced_df), model)\n",
    "                                for i, q in enumerate(balanced_df[\"question2\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KsajPEjAxgSh"
   },
   "outputs": [],
   "source": [
    "q1_duplicate_embeddings = question1_embeddings[duplicates.index]\n",
    "q2_duplicate_embeddings = question2_embeddings[duplicates.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iABdwDpYyePj"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(q1_duplicate_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nziyofClROj"
   },
   "outputs": [],
   "source": [
    "p_at_1_accuracy(cosine_distances_matrix, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-h3ZfJyKzeSM"
   },
   "outputs": [],
   "source": [
    "p_at_3_accuracy(cosine_distances_matrix, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CaePlc24znfS"
   },
   "outputs": [],
   "source": [
    "mrr(cosine_distances_matrix, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ms0vDrz4zx-b"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)\n",
    "tau_accuracy(cosine_distances_matrix, balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HafxZ9b65ljq"
   },
   "source": [
    "## Training Our Own CBOW ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wijHA8zmANzx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "text_corpus = pd.read_csv('balanced_df.csv')\n",
    "# text_corpus = pd.read_csv('tech_data.csv')\n",
    "train_corpus = text_corpus['question1'].sample(n=10500, random_state=35).tolist()\n",
    "test_corpus = text_corpus[~text_corpus['question1'].isin(train_corpus)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YP7MrloAOj5"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Lambda\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "corpus = train_corpus\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "sequences = tokenizer.texts_to_sequences(corpus)\n",
    "print(\"After converting our words in the corpus into vector of integers:\")\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rlYDRhqOAtHv"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define the parameters\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_size = 400\n",
    "window_size = 5\n",
    "\n",
    "# Generate the context-target pairs\n",
    "contexts = []\n",
    "targets = []\n",
    "for sequence in sequences:\n",
    "    for i in range(window_size, len(sequence) - window_size):\n",
    "        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n",
    "        target = sequence[i]\n",
    "        contexts.append(context)\n",
    "        targets.append(target)\n",
    "\n",
    "# Convert the contexts and targets to numpy arrays\n",
    "X = np.array(contexts)\n",
    "y = to_categorical(targets, num_classes=vocab_size)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the CBOW model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=2 * window_size))\n",
    "model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n",
    "model.add(Dense(units=vocab_size, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=200, verbose=0)\n",
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gi-KXFEbA3gT"
   },
   "outputs": [],
   "source": [
    "embedding_layer = model.layers[0]\n",
    "\n",
    "embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "# Function to obtain the sentence-level embedding by averaging word embeddings in the context\n",
    "def get_sentence_embedding(context):\n",
    "    word_embeddings = embedding_weights[context]\n",
    "\n",
    "    # Return the average of the embeddings for the context (you could also sum, or use other aggregations)\n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "\n",
    "context_example = X_test[0]\n",
    "sentence_embedding = get_sentence_embedding(context_example)\n",
    "\n",
    "print(\"Sentence Embedding (Vector for this context):\")\n",
    "print(sentence_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lqsr9WUwBCC4"
   },
   "outputs": [],
   "source": [
    "def get_sentence_embedding(context):\n",
    "    word_embeddings = embedding_weights[context]\n",
    "\n",
    "    return np.mean(word_embeddings, axis=0)\n",
    "\n",
    "def embed_new_sentence(new_sentence, tokenizer, model, window_size=2):\n",
    "\n",
    "    new_sequence = tokenizer.texts_to_sequences([new_sentence])[0]\n",
    "\n",
    "    if len(new_sequence) <= 2 * window_size:\n",
    "\n",
    "        return np.zeros((embedding_size,))\n",
    "\n",
    "\n",
    "    contexts = []\n",
    "    for i in range(window_size, len(new_sequence) - window_size):\n",
    "        context = new_sequence[i - window_size:i] + new_sequence[i + 1:i + window_size + 1]\n",
    "        contexts.append(context)\n",
    "\n",
    "    X_new = np.array(contexts)\n",
    "\n",
    "    embedding_layer = model.layers[0]\n",
    "    embedding_weights = embedding_layer.get_weights()[0]\n",
    "\n",
    "\n",
    "    sentence_embeddings = np.array([get_sentence_embedding(context) for context in X_new])\n",
    "\n",
    "    final_sentence_embedding = np.mean(sentence_embeddings, axis=0)\n",
    "\n",
    "    return final_sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O28DV8s5BvGa"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_cosine_similarity(embedding1, embedding2):\n",
    "    return cosine_similarity([embedding1], [embedding2])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jaLiOq_0BxVr"
   },
   "outputs": [],
   "source": [
    "# keyword_rows = test_corpus\n",
    "test_corpus = pd.read_csv('tech_data.csv')\n",
    "\n",
    "\n",
    "all_questions1 = keyword_rows['question1'].tolist()\n",
    "all_questions2 = keyword_rows['question2'].tolist()\n",
    "\n",
    "embeddings_question1 = np.array([\n",
    "    embed_new_sentence(q, tokenizer, model) for q in all_questions1\n",
    "])\n",
    "\n",
    "embeddings_question2 = np.array([\n",
    "    embed_new_sentence(q, tokenizer, model) for q in all_questions2\n",
    "])\n",
    "\n",
    "cosine_similarities = np.array([\n",
    "    compute_cosine_similarity(emb1, emb2)\n",
    "    for emb1, emb2 in zip(embeddings_question1, embeddings_question2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oVgvapgZBzVl"
   },
   "outputs": [],
   "source": [
    "max = 0\n",
    "print(\"Cosine Similarities for Question Pairs:\")\n",
    "for i, similarity in enumerate(cosine_similarities):\n",
    "\n",
    "    question1_text = all_questions1[i]\n",
    "    question2_text = all_questions2[i]\n",
    "\n",
    "    print(f\"Question 1: {question1_text}\")\n",
    "    print(f\"Question 2: {question2_text}\")\n",
    "    print(f\"Cosine Similarity: {similarity}\")\n",
    "    max +=1\n",
    "    if max == 10:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "349Bbt0wB1-Y"
   },
   "outputs": [],
   "source": [
    "correct_predictions = 0\n",
    "total_predictions = len(cosine_similarities)\n",
    "\n",
    "\n",
    "for i, similarity in enumerate(cosine_similarities):\n",
    "    is_duplicate = test_corpus.iloc[i]['is_duplicate']\n",
    "\n",
    "    if similarity > 0.8:\n",
    "        predicted_label = 1\n",
    "    else:\n",
    "        predicted_label = 0\n",
    "\n",
    "\n",
    "    if predicted_label == is_duplicate:\n",
    "        correct_predictions += 1\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "\n",
    "\n",
    "print(f\"Number of correct predictions: {correct_predictions}\")\n",
    "print(f\"Total number of comparisons: {total_predictions}\")\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLGK9-BhB46q"
   },
   "outputs": [],
   "source": [
    "correct_matches = 0\n",
    "total_checked = 20\n",
    "\n",
    "for i in range(min(total_checked, len(cosine_similarities))):\n",
    "    is_duplicate = test_corpus.iloc[i]['is_duplicate']\n",
    "\n",
    "    question1 = all_questions1[i]\n",
    "    question2 = all_questions2[i]\n",
    "\n",
    "    emb1 = embeddings_question1[i]\n",
    "\n",
    "    similarities_with_others = np.array([compute_cosine_similarity(emb1, emb2) for emb2 in embeddings_question1 + embeddings_question2])\n",
    "\n",
    "    similarities_with_others[i] = -1\n",
    "    most_similar_index = np.argmax(similarities_with_others)\n",
    "    most_similar_similarity = similarities_with_others[most_similar_index]\n",
    "\n",
    "    if most_similar_index < len(embeddings_question1):\n",
    "        most_similar_question = all_questions1[most_similar_index]\n",
    "    else:\n",
    "        most_similar_question = all_questions2[most_similar_index - len(embeddings_question1)]\n",
    "\n",
    "    if most_similar_index == i + len(embeddings_question1):\n",
    "        correct_matches += 1\n",
    "        correct_match_status = \"Correct\"\n",
    "    else:\n",
    "        correct_match_status = \"Incorrect\"\n",
    "\n",
    "    print(f\"Question 1: {question1}\")\n",
    "    print(f\"Question 2: {question2}\")\n",
    "    print(f\"Cosine Similarity with most similar question: {most_similar_similarity:.4f}\")\n",
    "    print(f\"Most Similar Question: {most_similar_question}\")\n",
    "    print(f\"True Label (is_duplicate): {is_duplicate}\")\n",
    "    print(f\"Prediction: {correct_match_status}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5n3M4MCCB7Oz"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "## Try top 3\n",
    "cosine_distances_matrix = cosine_distances(embeddings_question1, embeddings_question2)\n",
    "top_3_matches = np.argsort(cosine_distances_matrix, axis=1)[:, :3]\n",
    "correct_matches = np.array([i in top_3_matches[i] for i in range(len(keyword_rows))])\n",
    "top_3_accuracy = np.mean(correct_matches)\n",
    "print(f\"P@3: {top_3_accuracy:.2f}\")\n",
    "\n",
    "correct_ranks = []\n",
    "for i in range(len(cosine_similarities)):\n",
    "    sorted_indices = np.argsort(cosine_similarities)\n",
    "    rank = np.where(sorted_indices == i)[0]\n",
    "    if len(rank) > 0:\n",
    "        correct_ranks.append(rank[0] + 1)\n",
    "    else:\n",
    "        correct_ranks.append(float('inf'))\n",
    "\n",
    "# P@1\n",
    "p_at_1 = np.mean([1 if rank == 1 else 0 for rank in correct_ranks])\n",
    "\n",
    "# MRR\n",
    "mrr = np.mean([1 / rank for rank in correct_ranks if rank != float('inf')])\n",
    "\n",
    "\n",
    "print(f\"P@1: {p_at_1:.2f}\")\n",
    "print(f\"MRR: {mrr:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU4aSr7yabq-"
   },
   "source": [
    "## Trying Binary Classification Approach -- As Is (No Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uTHF51QraiFC"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nr5ZrDnqbSeL"
   },
   "outputs": [],
   "source": [
    "class QuestionPairDataset(Dataset):\n",
    "  def __init__(self, df, tokenizer):\n",
    "    self.df = df\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.df)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    row = self.df.iloc[index]\n",
    "    question1 = row['question1']\n",
    "    question2 = row['question2']\n",
    "    label = row['is_duplicate']\n",
    "\n",
    "    inputs = self.tokenizer(\n",
    "        text=question1,\n",
    "        text_pair=question2,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        add_special_tokens=True,\n",
    "        return_tensors=\"pt\")\n",
    "\n",
    "    return {'input_ids': inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SX3Ar2E3c0kh"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader):\n",
    "  model.eval()\n",
    "  predictions = []\n",
    "  true_labels = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "      input_ids = batch['input_ids'].to(device)\n",
    "      attention_mask = batch['attention_mask'].to(device)\n",
    "      labels = batch['labels'].to(device)\n",
    "\n",
    "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "      predictions.extend(preds.cpu().numpy())\n",
    "      true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "  return predictions, true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z-o3hSMQivoy"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_axo9BU0dhE0"
   },
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "baseline_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKza3TDQdmIN"
   },
   "source": [
    "For tech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKsTvBrhdlWv"
   },
   "outputs": [],
   "source": [
    "test_dataset = QuestionPairDataset(tech_data, tokenizer)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "baseline_predictions, baseline_true_labels = evaluate(baseline_model, test_loader)\n",
    "baseline_accuracy = accuracy_score(baseline_true_labels, baseline_predictions)\n",
    "baseline_report = classification_report(baseline_true_labels, baseline_predictions)\n",
    "\n",
    "print(f\"Baseline Test Accuracy: {baseline_accuracy}\")\n",
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rgpc9P8adc-3"
   },
   "source": [
    "For general dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqLrJz-JcXRc"
   },
   "outputs": [],
   "source": [
    "test_dataset = QuestionPairDataset(balanced_df, tokenizer)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "baseline_predictions, baseline_true_labels = evaluate(baseline_model, test_loader)\n",
    "baseline_accuracy = accuracy_score(baseline_true_labels, baseline_predictions)\n",
    "baseline_report = classification_report(baseline_true_labels, baseline_predictions)\n",
    "\n",
    "print(f\"Baseline Test Accuracy: {baseline_accuracy}\")\n",
    "print(baseline_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB13Tsn8buvI"
   },
   "source": [
    "## Trying Binary Classification Approach -- With Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fe0eiYt6fye_"
   },
   "outputs": [],
   "source": [
    "fined_tuned_model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-qLV0VCgalJ"
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(tech_data, test_size=0.3, random_state=42)\n",
    "\n",
    "train_dataset = QuestionPairDataset(train_data, tokenizer)\n",
    "test_dataset = QuestionPairDataset(test_data, tokenizer)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eliJCY1UcPNg"
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(fined_tuned_model.parameters(), lr=2e-5)\n",
    "epochs = 3\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  epoch_loss = 0\n",
    "  correct_predictions = 0\n",
    "  total = 0\n",
    "  fined_tuned_model.train()\n",
    "\n",
    "  for batch in train_loader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    outputs = fined_tuned_model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    epoch_loss += loss.item()\n",
    "\n",
    "    logits = outputs.logits\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    correct_predictions += (preds == labels).sum().item()\n",
    "    total += labels.size(0)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  train_losses.append(epoch_loss / len(train_loader))\n",
    "  train_accuracies.append(correct_predictions / total)\n",
    "\n",
    "  print(f\"Epoch loss: {train_losses[-1]}, Epoch accuracy: {train_accuracies[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yfh8gn0Mfi6b"
   },
   "outputs": [],
   "source": [
    "fine_tuned_predictions, fine_tuned_true_labels = evaluate(fined_tuned_model, test_loader)\n",
    "fine_tuned_accuracy = accuracy_score(fine_tuned_true_labels, fine_tuned_predictions)\n",
    "fine_tuned_report = classification_report(fine_tuned_true_labels, fine_tuned_predictions)\n",
    "\n",
    "print(f\"Fine-Tuned Test Accuracy: {fine_tuned_accuracy}\")\n",
    "print(fine_tuned_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7v-9mB81SfLo"
   },
   "source": [
    "## Trying BERT w/ Cosine Distance -- As Is (No Fine-Tuning) ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1itgZ24VSi5X"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Pre-trained BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pmx4n6SPpFq2"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhFSdiJZpfHM"
   },
   "outputs": [],
   "source": [
    "def cls_embeddings(questions, tokenizer, model):\n",
    "  embeddings = []\n",
    "  with torch.no_grad():\n",
    "    for question in questions:\n",
    "      inputs = tokenizer(text = question,\n",
    "               truncation = True,\n",
    "               padding = True,\n",
    "               add_special_tokens = True,\n",
    "               return_tensors = \"pt\")\n",
    "\n",
    "      input_ids = inputs[\"input_ids\"]\n",
    "      attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "      last_hidden_state = outputs[0]\n",
    "      cls_representation = last_hidden_state[:,0,:]\n",
    "      embeddings.append(cls_representation.cpu().numpy())\n",
    "\n",
    "  return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GSwZLjTbmI66"
   },
   "source": [
    "For the tech dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3bAtf-7sv0K"
   },
   "outputs": [],
   "source": [
    "question1_embeddings = cls_embeddings(tech_data[\"question1\"], tokenizer, bert_model)\n",
    "question2_embeddings = cls_embeddings(tech_data[\"question2\"], tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6ZJy2c8Vs8bF"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DJswEsfzUfHr"
   },
   "outputs": [],
   "source": [
    "duplicates = tech_data[tech_data[\"is_duplicate\"] == 1]\n",
    "duplicate_indices = duplicates.index.tolist()\n",
    "cosine_distances_duplicates = cosine_distances_matrix[duplicate_indices, :]\n",
    "\n",
    "p_at_1_accuracy(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tbjVqzROtv92"
   },
   "outputs": [],
   "source": [
    "p_at_3_accuracy(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bmCVwZBMup2C"
   },
   "outputs": [],
   "source": [
    "mrr(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNRAOlXHvQw-"
   },
   "outputs": [],
   "source": [
    "tau_accuracy(cosine_distances_matrix, tech_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KKiJaXexZmF"
   },
   "source": [
    "For the general dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Otvqsd5Qxcl1"
   },
   "outputs": [],
   "source": [
    "question1_embeddings = cls_embeddings(balanced_df[\"question1\"], tokenizer, bert_model)\n",
    "question2_embeddings = cls_embeddings(balanced_df[\"question2\"], tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRRXbGe7xjGu"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quEEB_ajxnT3"
   },
   "outputs": [],
   "source": [
    "# Getting P@1\n",
    "duplicates = balanced_df[balanced_df[\"is_duplicate\"] == 1]\n",
    "duplicate_indices = duplicates.index.tolist()\n",
    "cosine_distances_duplicates = cosine_distances_matrix[duplicate_indices, :]\n",
    "\n",
    "p_at_1_accuracy(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIzCyB9Dy3va"
   },
   "outputs": [],
   "source": [
    "# Getting P@3\n",
    "p_at_3_accuracy(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8miDDoV1y47U"
   },
   "outputs": [],
   "source": [
    "# Getting MRR\n",
    "mrr(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kPjjzvE6y_HO"
   },
   "outputs": [],
   "source": [
    "tau_accuracy(cosine_distances_matrix, balanced_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gH62lX2dXH36"
   },
   "source": [
    "## Trying BERT w/ Cosine Distance -- With Fine-Tuning ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBPG-A_azmCK"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "\n",
    "# Source\n",
    "# https://huggingface.co/docs/transformers/en/tasks/masked_language_modeling\n",
    "\n",
    "class TokenizeQuestion(Dataset):\n",
    "  def __init__(self, q, tokenizer, max_length=128):\n",
    "    self.questions = q\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.questions)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    quest = self.questions[idx]\n",
    "    tokens = self.tokenizer(text = quest, truncation = True, padding= True, add_special_tokens = True, return_tensors=\"pt\")\n",
    "    return {key: val.squeeze(0) for key, val in tokens.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4JcEIcvKHX8"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1XYhL6J2KOT5"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "bert_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xTzfl71YKTkD"
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_mlm\",\n",
    "    eval_strategy=\"no\",\n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eP2xbAteIrW4"
   },
   "outputs": [],
   "source": [
    "def cls_embeddings(questions, tokenizer, model):\n",
    "  embeddings = []\n",
    "  with torch.no_grad():\n",
    "    for question in questions:\n",
    "      inputs = tokenizer(text = question,\n",
    "               truncation = True,\n",
    "               padding = True,\n",
    "               add_special_tokens = True,\n",
    "               return_tensors = \"pt\")\n",
    "\n",
    "      input_ids = inputs[\"input_ids\"]\n",
    "      attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "      input_ids = input_ids.to(device)\n",
    "      attention_mask = attention_mask.to(device)\n",
    "\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "      last_hidden_state = outputs[0]\n",
    "      cls_representation = last_hidden_state[:,0,:]\n",
    "      embeddings.append(cls_representation.cpu().numpy())\n",
    "\n",
    "  return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-ZrC14sKZA5"
   },
   "source": [
    "For tech dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mHkBmg8NU7W"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(tech_data, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EJFa20sMKb5a"
   },
   "outputs": [],
   "source": [
    "train_questions = train_df[\"question1\"].tolist() + train_df[\"question2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dAnq2RGHKeWQ"
   },
   "outputs": [],
   "source": [
    "dataset = TokenizeQuestion(train_questions, tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm = True, mlm_probability = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ8V_QamKv-4"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0TemW24IVI7"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2g4PnAsIJsf"
   },
   "outputs": [],
   "source": [
    "question1_embeddings = cls_embeddings(test_df[\"question1\"], tokenizer, bert_model)\n",
    "question2_embeddings = cls_embeddings(test_df[\"question2\"], tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W3JZmNsKBG5"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sSeGI24oMZtA"
   },
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index(drop=True)\n",
    "duplicates = test_df[test_df[\"is_duplicate\"] == 1]\n",
    "duplicate_indices = duplicates.index.tolist()\n",
    "cosine_distances_duplicates = cosine_distances_matrix[duplicate_indices, :]\n",
    "\n",
    "p_at_1_accuracy(cosine_distances_duplicates, duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9T3lEHH5Mhc4"
   },
   "outputs": [],
   "source": [
    "tau_accuracy(cosine_distances_matrix, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNrPmuraSE2w"
   },
   "source": [
    "For general dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5R27yAQZ7qu"
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(balanced_df, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpuctXApZ9I5"
   },
   "outputs": [],
   "source": [
    "train_questions = train_df[\"question1\"].tolist() + train_df[\"question2\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h69u3t48aAkC"
   },
   "outputs": [],
   "source": [
    "dataset = TokenizeQuestion(train_questions, tokenizer)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm = True, mlm_probability = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qECvzxTQaDTj"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvERtTX6aGjo"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqLucpVGaJaI"
   },
   "outputs": [],
   "source": [
    "question1_embeddings = cls_embeddings(test_df[\"question1\"], tokenizer, bert_model)\n",
    "question2_embeddings = cls_embeddings(test_df[\"question2\"], tokenizer, bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "womlEV36aL_O"
   },
   "outputs": [],
   "source": [
    "cosine_distances_matrix = cosine_distances(question1_embeddings, question2_embeddings)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
